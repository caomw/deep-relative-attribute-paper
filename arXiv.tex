\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{mathrsfs}
\usepackage{soul}
\usepackage{multirow}
\usepackage{comment}
\usepackage{tabularx}
\usepackage[tableposition=top]{caption}
\usepackage{pbox}

%\usepackage[sort&compress,square,comma]{natbib}

\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{filecontents,pgfplots}
\usepackage{tikz}
\usetikzlibrary{shapes,matrix,arrows,decorations.pathmorphing, positioning, calc, decorations.markings}
\usepackage{sci}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


% Include other packages here, before hyperref.
\usepackage{stackengine}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{725} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Deep Relative Attributes}

\author{Yaser Souri$^1$, Erfan Noury$^2$, Ehsan Adeli-Mosabbeb$^3$\\
$^1$Sobhe \quad $^2$Sharif University of Technology \quad $^3$University of North Carolina at Chapel Hill\\
{\tt\small souri@sobhe.ir, erfan.noury@live.com, eadeli@unc.edu}
%\and
%Erfan Noury\\
%Sharif University of Technology\\
%{\tt\small noury@ce.sharif.edu}
%\and
%Ehsan Adeli Mosabbeb\\
%University of North Carolina at Chapel Hill\\
%{\tt\small eadeli@unc.edu}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Visual attributes are great means of describing images or scenes, in a way both humans and computers understand. In order to establish a correspondence between images and to be able to compare the strength of each property between images, relative attributes were introduced. However, since their introduction, hand-crafted and engineered features were used to learn complex models for the problem of relative attributes. This limits the applicability of those methods for more realistic cases.
% In this paper, we propose a deep neural network strategy to rank images, based on a set of learned attribute-specific features.
We introduce a two part deep learning architecture for the task of relative attribute prediction.
A convolutional neural network (ConvNet) architecture is adopted to learn the features with addition of an additional layer (ranking layer) that learns to rank the images based on these features. Also an appropriate ranking loss is adapted to train the whole network in an end-to-end fashion.
% Therefore, our proposed model enjoys a unified deep neural network model, in which both feature learning and ranking are intertwined, influencing each other while enhancing the overall performance.
Our proposed method outperforms the baseline and state-of-the-art methods in relative attribute prediction on various datasets. Our qualitative results also show that the network is able to learn effective features for the task. Furthermore, we use our trained models to visualize saliency maps for each attribute.
\end{abstract}


\input{arxiv-1Introduction}
\input{arxiv-2RelatedWorks}
\input{arxiv-3ProposedMethod}
\input{arxiv-4Experiments}
\input{arxiv-5Conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{refs}
}

\end{document}
