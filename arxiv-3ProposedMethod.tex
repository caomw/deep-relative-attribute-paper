
% !TeX root=arXiv.tex
% !TEX TS-program = pdfLatex

%%%%%%%%%%%%%%%%%%%%%%% END-TO-END DEEP RELATIVE ATTRIBUTES %%%%%%%%%%%%%%%
\section{Proposed Method}
\label{sec.3}

We propose to use a ConvNet based deep neural network that is trained to optimize an appropriate ranking loss for the task of predicting relative attribute strength. The network architecture consists of two parts, the \textit{feature learning and extraction} part and the \textit{ranking} part.

The feature learning and extraction part takes a fixed size image, $I_i$, as input and outputs the learned feature representation for that image $\psi_i \in \mathbb{R}^d$.
Different network architectures have been developed in the literature over the past few years, for extracting and leaning features in a deep framework that almost all of them are applicable. For our purpose, the outputs of some intermediate layer of a state-of-the-art ConvNet architecture, such as the last layer before the probability layer of AlexNet \cite{krizshevsky}, VGGNet \cite{verydeep} or GoogleNet \cite{googlenet},
can be used.

One of the most widely used models for relative attributes in the literature is RankSVM \cite{Joachims2002}. However, here, we want a neural network based ranking procedure that accepts relatively ordered pairs of feature vectors as input during training, and learns to map each feature vector to an absolute ranking for testing purposes. Burges \etal~\cite{Burges2005} introduced such a neural network based ranking procedure with our desired properties. The ranking part of our proposed network architecture is thus similar (reffered to as RankNet).

During training for a minibatch of image pairs and their target orderings, the output of the feature learning and extraction part of the network is fed into the ranking part and a ranking loss is computed. The loss is then back-propagated through the network which enables us to simultaneously learn the parameters of feature extraction (ConvNet) and ranking (RankNet). Further with simple techniques we can generate saliency maps for each attribute (see section \ref{sec.4.5}). These saliency maps exhibit interesting properties as they can be used to localize the pixels in the image where the attribute is present.

\subsection{RankNet: Learning to Rank Using Gradient Descent}\label{sec3.1}
This section briefly overviews the RankNet \cite{Burges2005} procedure in our context.
Given a set of pairs of sample feature vectors $\big\{( \psi_{1}^{(k)}, \psi_{2}^{(k)} ) | k \in \{1, ...,n\} \big\} \in \mathbb{R}^{d \times d}$ and target probabilities $\big\{ t_{12}^{(k)} | k \in \{1, ...,n\} \big\}$, that sample $\psi_{1}^{(k)}$ is to be ranked higher than sample $\psi_{2}^{(k)}$, we want to learn a ranking function $f : \mathbb{R}^d \mapsto \mathbb{R}$, such that $f$ specifies the ranking order of a set of features. Here, $f(\psi_i) > f(\psi_j)$ indicates that the model thinks that feature vector $\psi_i$ is ranked higher than $\psi_j$, denoted by $\psi_i \triangleright \psi_j$. The RankNet model \cite{Burges2005} provides an elegant neural network based way to learn the function $f$.

Denoting $r_i \equiv f(\psi_i)$, RankNet models the mapping from rank estimates to posterior probabilities $p_{ij} = P(\psi_i \triangleright \psi_j)$ using a logistic function 
$$
p_{ij} \equiv \frac{1}{1 + e^{-(r_i - r_j)}}.
$$

The loss for the sample pair of feature vectors $(\psi_i, \psi_j)$ along with target probability $t_{ij}$ is defined as
$$
C_{ij} \equiv - t_{ij} \log (p_{ij}) - (1 - t_{ij}) \log (1 - p_{ij}),
$$
which is the binary cross entropy loss.
Figure \ref{fig.2} plots the loss value $C_{ij}$ as a function of $r_i - r_j$ for three values of target probability $t_{ij} = \{0, 0.5, 1\}$.

Specifically we don't do regression instead of the ranking for two reasons. First we cannot regress the absolute rank of images because annotation is only available in pairwise ordering for each attribute in relative attribute datasets (see section \ref{sec.4.1}). Second regressing the difference $r_i - r_j$ to $t_{ij}$ is also inappropriate.
Let's consider the squared loss
$$
R_{ij} = \big[(r_i - r_j) - t_{ij}\big]^2
$$
which is typically used for regression (Figure \ref{fig.2}). We see that the regression loss forces the difference of rank estimates to be a specific value and disallows over estimation. Further more its quadratic natures makes it very sensitive to noise. This shed light into why regression objective is the wrong objective to optimize when the goal is ranking.

\begin{figure}
    \centering
    \begin{subfigure}
        \centering
        \includegraphics[width=4cm]{fig2-2/fig3.eps}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics[width=4cm]{fig2-2/fig3-reg.eps}
    \end{subfigure}
    \caption{The ranking loss value for three values of the target probability (left). The square loss value for three values of the target probability, typically used for regression (right).}
    \label{fig.2}
\end{figure}

Note that when $t_{ij} = 0.5$, and no information is available about the relative rank of the two samples, the ranking cost becomes symmetric. This can be used as a way to train on patterns that are desired to have similar rank. This is somewhat scarce in the previous works on relative attributes.
Furthermore this model asymptotes to a linear function which makes it more appropriate for problems with noisy labels. %than quadratic loss functions.

Training this model is possible using stochastic gradient descent or its variants like RMSProp.%, which is also used in our proposed method.
While testing, we only need to estimate the value of $f(\psi_i)$, which resembles the absolute rank of the test sample. Using $f(\psi_i)$s we can easily infer the relative ordering or absolute ordering of test pairs.

\subsection{Deep Relative Attributes}\label{sec3.2}

%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}
\centering
\scalebox{.3}
{
% We need layers to draw the block diagram
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

% Define a few styles and constants
\tikzstyle{sensor}=[draw, fill=blue!20, text width=5em, 
    text centered, minimum height=2.5em]
\tikzstyle{ann} = [above, text width=5em]
\tikzstyle{naveqs} = [sensor, text width=6em, fill=red!20, 
    minimum height=12em, rounded corners]
\def\blockdist{2.3}
\def\edgedist{2.5}

\begin{tikzpicture}
    % feature extraction part rectangle
%	\node [scale=3] (testtitle) at (10.5cm, 5.0cm) {\textcolor{red}{Test time}};
	\draw [rounded corners=1cm, dashed, line width=3, red] (-3.5cm, 4cm) rectangle (24.cm,-2.5cm);
	% images
	\node (im1) at (0cm,1cm) [draw] {\includegraphics[scale=1]{Fig2/im1.jpg}};
	\node (im2) at (0cm, -6cm) [draw] {\includegraphics[scale=1]{Fig2/im2.jpg}};

	% topconv1 layer
	\node (tconv1) at (5.1cm, 1cm) {};
%	\draw [fill=blue!20] (5.6cm,3.1cm) rectangle (9.6cm,0.1cm);
	\draw [fill=blue!20] (5.4cm,2.9cm) rectangle (9.4cm,-0.1cm);
	\draw [fill=blue!20] (5.2cm,2.7cm) rectangle (9.2cm,-0.3cm);
	\draw [fill=blue!20] (5cm,2.5cm) rectangle (9cm,-0.5cm);

	% bottomconv1 layer
	\node (bconv1) at (5.1cm, -6cm) {};
%	\draw [fill=blue!20] (5.6cm,-3.9cm) rectangle (9.6cm,-6.9cm);
	\draw [fill=blue!20] (5.4cm,-4.1cm) rectangle (9.4cm,-7.1cm);
	\draw [fill=blue!20] (5.2cm,-4.3cm) rectangle (9.2cm,-7.3cm);
	\draw [fill=blue!20] (5cm,-4.5cm) rectangle (9cm,-7.5cm);

	% arrows from images to conv1s
	\path [draw, ->, line width=3] (im1.east) -- node [above, scale=3] {$I_i$} (tconv1);
	\path [draw, ->, line width=3] (im2.east) -- node [above, scale=3] {$I_j$} (bconv1);

	% topconv2 layer
	\node (tconv2) at (12cm, 1cm) {};
	\draw [fill=blue!20] (12.6cm,3.1cm) rectangle (16.6cm,0.1cm);
	\draw [fill=blue!20] (12.4cm,2.9cm) rectangle (16.4cm,-0.1cm);
	\draw [fill=blue!20] (12.2cm,2.7cm) rectangle (16.2cm,-0.3cm);
	\draw [fill=blue!20] (12cm,2.5cm) rectangle (16cm,-0.5cm);

	% bottomconv2 layer
	\node (bconv2) at (12cm, -6cm) {};
	\draw [fill=blue!20] (12.6cm,-3.9cm) rectangle (16.6cm,-6.9cm);
	\draw [fill=blue!20] (12.4cm,-4.1cm) rectangle (16.4cm,-7.1cm);
	\draw [fill=blue!20] (12.2cm,-4.3cm) rectangle (16.2cm,-7.3cm);
	\draw [fill=blue!20] (12cm,-4.5cm) rectangle (16cm,-7.5cm);

	\path (tconv1.east)+(4.65cm, 0cm) -- node [scale=4]{\dots} (tconv2);
	\path (bconv1.east)+(4.65cm, 0cm) --node [scale=4]{\dots} (bconv2);

	\node (convnet) [below, scale=3.5] at (11cm, -8cm) {ConvNet};

	% toprank
	\node (tconvout) at (16.6cm, 1cm) {};
	\node (trank) at (21.1cm, 1cm) [rectangle, draw, fill=red!20, scale=3, align=center] {Ranking \\  Layer};
	\path [draw, line width=3, ->] (tconvout) -- node[above, scale=3] {$\psi_i$} (trank.west);

	% bottomrank
	\node (bconvout) at (16.6cm, -6cm) {};
	\node (brank) at (21.1cm, -6cm) [rectangle, draw, fill=red!20, scale=3, align=center] {Ranking \\  Layer};
	\path [draw, line width=3, ->] (bconvout) -- node[above, scale=3] {$\psi_j$} (brank.west);

	% shared rank layer
	\node (dashcenter) at (10.75cm, 0cm) {};
	\path [draw, line width=2, <->] (trank.south) -- node [scale=2, draw, rectangle, fill=white, line width=0] {\rotatebox{90}{shared}} (brank.north);
	\path [draw, line width=2, <->] (trank.south -| dashcenter) -- node [scale=2, draw, rectangle, fill=white, line width=0] {\rotatebox{90}{shared}} (brank.north -| dashcenter);

	% posterior
	\node (posterior) at (27cm, -2.5cm) [rectangle, draw, fill=orange!10, scale=3, minimum width=15] {\rotatebox{90}{Posterior}};

	% connect rank layer to posterior
	\path [draw, line width=2, ->] (trank.east) -- node [scale=3, above] {$r_i$} (posterior);
	\path [draw, line width=2, ->] (brank.east) -- node [scale=3, above] {$r_j$} (posterior);

	% BXE
	\node (bxent) at (36cm, -2.5cm) [draw,fill=green!10, regular polygon, regular polygon sides=3, shape border rotate=-90, scale=8, line width=2] {};
	\node at (36.2cm, -2.5cm) {\Huge BXEnt};

	% target value
	\node (target) at (29.5cm , -4.2cm) [scale=3] {target};

	% connect to bxe
	\draw [line width=3, ->] (posterior.east) -- ++(3cm, 0cm) |- node [above right, scale=3] {$p_{ij}$}  (bxent.north west);
	\draw [line width=3, ->] (target.east) -- node [below, scale=3] {$t_{ij}$}  (bxent.south west |- target.east);

	% loss node
	\node (loss) [scale=3] at (45cm, -2.5cm) { loss};

	% connect bxent to loss
	\draw [line width=3, ->] (bxent.east) -- node [above, scale=3] {$C_{ij}$}  (loss.west);
\end{tikzpicture}}
\caption{The overall schematic view of the proposed method during training. Pairs of the images are presented to the network with their corresponding target probabilities. The network consists of two parts, the \textit{feature learning and extraction} part (labeled ConvNet in the figure), and the \textit{ranking} part (the Ranking Layer).}
\label{fig.3}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%% Figure 3 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
\centering
\scalebox{.3}
{
% We need layers to draw the block diagram
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

% Define a few styles and constants
\tikzstyle{sensor}=[draw, fill=blue!20, text width=5em, 
    text centered, minimum height=2.5em]
\tikzstyle{ann} = [above, text width=5em]
\tikzstyle{naveqs} = [sensor, text width=6em, fill=red!20, 
    minimum height=12em, rounded corners]
\def\blockdist{2.3}
\def\edgedist{2.5}

\begin{tikzpicture}
	\node (im1) at (0cm,1cm) [draw] {\includegraphics[scale=1]{Fig1/pair3A.jpg}};

	% topconv1 layer
	\node (tconv1) at (5.1cm, 1cm) {};
	\draw [fill=blue!20] (5.4cm,2.9cm) rectangle (9.4cm,-0.1cm);
	\draw [fill=blue!20] (5.2cm,2.7cm) rectangle (9.2cm,-0.3cm);
	\draw [fill=blue!20] (5cm,2.5cm) rectangle (9cm,-0.5cm);

	% arrows from images to conv1s
	\path [draw, ->, line width=3] (im1.east) -- node [above, scale=3] {$I_i$} (tconv1);

	% topconv2 layer
	\node (tconv2) at (12cm, 1cm) {};
	\draw [fill=blue!20] (12.6cm,3.1cm) rectangle (16.6cm,0.1cm);
	\draw [fill=blue!20] (12.4cm,2.9cm) rectangle (16.4cm,-0.1cm);
	\draw [fill=blue!20] (12.2cm,2.7cm) rectangle (16.2cm,-0.3cm);
	\draw [fill=blue!20] (12cm,2.5cm) rectangle (16cm,-0.5cm);

	\path (tconv1.east)+(4.65cm, 0cm) -- node [scale=4]{\dots} (tconv2);

	% toprank
	\node (tconvout) at (16.6cm, 1cm) {};
	
	\node (trank) at (20.5cm, 1cm) [rectangle, draw, fill=red!20, scale=3, align=center] {Ranking \\  Layer};
	\path [draw, line width=3, ->] (tconvout) -- node[above, scale=3] {$\psi_i$} (trank.west);
	
	% posterior
	\node (posterior) at (24cm, 1cm){};

	% connect rank layer to posterior
	\path [draw, line width=3, ->] (trank.east) -- node [scale=3, above] {$r_i$} (posterior);

	
\end{tikzpicture}}
\caption{During testing we only need to evaluate $r_i$ for each test image. From that we can easily infer the relative or absolute ordering of test images regarding to the attribute we have trained our model on.}
\label{fig.4}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%% Figure 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our proposed model is depicted in figure \ref{fig.3}. The model is trained separately, for each attribute. During training, pairs of images $(I_i, I_j)$ are presented to the network, together with the target probability $t_{ij}$. If for the attribute $I_i \triangleright I_j$ (image $i$ exhibits more of the attribute than image $j$), then $t_{ij}$ is expected to be larger than $0.5$ depending on our confidence on the relative ordering of $I_i$ and $I_j$. Similarly, if $I_i \triangleleft I_j$, then $t_{ij}$ is expected to be smaller than $0.5$, and if it is desired that the two images have the same rank, $t_{ij}$ is expected to be $0.5$. Because of the nature of the datasets we chose $t_{ij}$ from the set $\{1, 0.5, 0 \}$, according to the available annotations in the dataset.

The pair of images, then go though the feature learning and extraction part of the network (ConvNet). This procedure maps the images into feature vectors $\psi_i$ and $\psi_j$. Afterwards, these feature vectors go through the ranking layer, as described in section \ref{sec3.1}. We choose the ranking layer to be a fully connected neural network layer with linear activation function, a single output neuron and parameters $w$ and $b$. It maps the feature vector $\psi_i$ to the estimated absolute rank of that feature vector $r_i$, where
$$
r_i \equiv w^T x + b
$$
and $r_i \in \mathbb{R}$.
The two estimate ranks are then combined to output the estimated posterior probability $P(\psi_i \triangleright \psi_j)$ and is used along with the target probability $t_{ij}$ to calculate the loss. This loss is then back-propagated through the network and is used to update the parameters of the whole network, including both the parameters of the feature learning and extraction sub-network and the ranking layer.

During test time, as shown in Figure \ref{fig.4}, we only need to calculate the estimated absolute rank $r_i$ for each test image $I_i$. Using these estimated absolute ranks we can then easily infer the relative or absolute attribute ordering, for all test pairs.