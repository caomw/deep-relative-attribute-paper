
% !TeX root=DeepRelAttr.tex
% !TEX TS-program = pdfLatex

%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%  After the "Relative Attributes" paper \cite{parikh2011}, there was a stream of papers that aimed to solve the same or similar task (\cite{Li2013,Yu2014,Sandeep_2014_CVPR,Lee_2013_7540}) using a different and often more complex model instead of the original RankSVM model. I think as progress in the "Visual Recognition" field has shown, for solving the problem you actually need to change the features not the model. So in this project I actually want to experiment with how learning the features end-to-end (or fine-tuning the features) can improve Relative Attributes accuracy and power. 
 
%  Please follow the notations as in here\footnote{\notation}. We will remove this footnote. I just put it here, so that we all know how to write the formulations. Use \

Visual attributes are linguistic terms that bear semantic properties of (visual) entities, often shared among categories. They are both human understandable and machine detectable, which makes them appropriate for better human machine communications. Visual attributes have been successfully used for many applications, such as image search \cite{pubfig, whittlesearch}, interactive fine-grained recognition \cite{branson10, branson13} and zero-shot learning \cite{6571196, parikh2011}.

Traditionally, visual attributes were treated as binary concepts \cite{ferrari2007learning, Farhadi09describingobjects}, as if they are present or not, in an image or a scene. Parikh and Grauman ~\cite{parikh2011} introduced a more natural view on visual attributes, in which pairs of visual entities can be compared, with respect to their relative strength of any specific property. With a set of human assessed relative orderings of image pairs, they learn a global ranking function for each attribute (Figure \ref{fig.1}).
While binary visual attributes relate properties to entities (\eg, a dog being furry), relative attributes make it possible to relate entities to each other in terms of their properties (\eg, bunnies being furrier than dogs).

Many have tried to build on the seminal work of Parikh and Grauman \cite{parikh2011} with more complex and task specific models for ranking, while still using hand crafted visual features, such as GIST \cite{Aude01} and HOG \cite{hog}. Recently, Convolutional Neural Networks (ConvNets) have proved successful in various visual recognition tasks, such as image classification \cite{krizshevsky}, object detection \cite{RCNN} and image segmentation \cite{fullyconv}. Many attribute the success of ConvNets to their ability to learn multiple layers of visual features from data. 

In this work, we propose to use a ConvNet-based architecture to learn the ranking of images, based on the relatively annotated images on each single attribute. a set of pairs of images depicting similar and/or different strengths of some particular attribute. The network uses the ConvNet to learn a series of visual features, which are known to work better than engineered visual features \cite{offtheshelf}. However, after learning the features, we further need to rank the images. We propose to add another layer to the network, to rank the images based on the learned features. The ranking layer could be learned through gradient descent (described in more details later). As a result, it is possible to learn (or fine-tune) the features while learning the ranking layer using back-propagation. Interweaving the two processes leads to a set of learned features that characterize each single attribute. This increases the overall performance and is the main advantage of our proposed method over previous methods. It is noteworthy to pinpoint that by exploiting the saliency maps of the unsupervised learned features for each attribute, similar to \cite{simonyan14a}, we can discover the spatial extent \cite{specialextent} of the attribute. %This is done using the same network in an unsupervised manner.

To our knowledge, our approach improves the state of the art or achieves very competitive results on all major datasets publicly available for relative attributes, both coarse-grained and fine-grained.

The rest of the paper is constructed as follows: Section 2 discusses the related work, Section 3 ....