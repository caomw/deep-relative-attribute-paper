
% !TeX root=arXiv.tex
% !TEX TS-program = pdfLatex

%%%%%%%%%%%%%%%%%%%%%%%%%% RELATED WORKS %%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Works}
\label{sec.2}

We usually describe visual concepts with their attributes.%, and how they look. 
Attributes are, therefore, mid-level representations for describing objects and scenes. In an early work on attributes, Farhadi \etal~\cite{Farhadi09describingobjects} proposed to describe objects using mid-level attributes. In another work \cite{farhadi10}, the authors described images based on %their attributes, basically 
a semantic triple "object, action, scene". In the recent years, attributes have shown great performance in object recognition \cite{Farhadi09describingobjects,7298613}, action recognition \cite{6838985,5995353} and event detection \cite{6475038}. Lampert \etal~\cite{6571196} predicted unseen objects using a zero-shot learning framework, in which binary attribute representation of the objects were incorporated. 

On the other hand, comparing attributes enables us to easily and reliably search through high-level data derived from \eg, documents or images. For instance, Kovashka \etal~\cite{KovashkaG13} proposed a relevance feedback strategy for image search using attributes and their comparisons. In order to establish the capacity for comparing attributes, we need to move from binary attributes towards describing attributes relatively. In the recent years, relative attributes have attracted the attention of many researchers.
%, in which a function is learned for each attribute that enables comparison between different attributes.
For instance, a linear relative comparison function is learned in \cite{parikh2011}, based on RankSVM \cite{Joachims2002} and a non-linear strategy in \cite{Li2013}. In another work, Datta \etal~\cite{5771429} used trained rankers for each facial image feature and formed a global ranking function for attributes.

Through the process of learning the attributes, different types of low-level image features are incorporated. For instance, Parikh and Grauman~\cite{parikh2011} used 512-dimensional GIST \cite{Aude01} descriptors as image features, while Jayaraman \etal~\cite{6909607} used histograms of image features, and reduced their dimensionality using PCA. Other works tried learning attributes through \eg, local learning \cite{1641014} or fine-grained comparisons \cite{Yu2014}. Yu and Grauman \cite{Yu2014} proposed a local learning-to-rank framework for fine-grained visual comparisons, in which the ranking model is learned using only analogous training comparisons. In another work \cite{Yu2015}, they proposed a local Bayesian model to rank images, which are hardly distinguishable for a given attribute. However, none of these methods leverage the effectiveness of feature learning methods and only use engineered and hand-crafted features for predicting relative attributes. %did to leverage the effectiveness of convolutional neural networks and feature learning for relative attribute prediction.

As could be inferred from the literature, it is very hard to decide what low-level image features to use for identifying and comparing visual attributes. Recent studies show that features learned through the convolutional neural networks (CNNs) \cite{NIPS1989_293} (also known as deep features) could achieve great performance for image classification \cite{NIPS2012_4824} and object detection \cite{6909475}. Zhang \etal~\cite{6909608} utilized CNNs for classifying binary attributes. In other works, Escorcia \etal~\cite{Escorcia_2015_CVPR} proposed CCNs with attribute centric nodes within the network for establishing the relationships between visual attributes. Shankar \etal~\cite{Shankar_2015_CVPR} proposed a weakly supervised setting on convolutional neural networks, applied for attribute detection. Khan \etal~\cite{khan15} used deep features for describing human attributes and thereafter for action recognition, and Huang \etal~\cite{Huang_2015_ICCV} used deep features for cross-domain image retrieval based on binary attributes.

Neural networks have also been extended for learning-to-rank applications. One of the earliest networks for ranking was proposed by Burges\etal~\cite{Burges2005}, known as RankNet. The underlying model in RankNet maps an input feature vector to a Real number. The model is trained  by presenting the network pairs of input training feature vectors with differing labels. Then, based on how they should be ranked, the underlying model parameters are updated. This model is used in different fields for ranking and retrieval applications, \eg, for personalized search \cite{song2014} or content-based image retrieval \cite{Wan2014}. 

